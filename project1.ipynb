{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371208f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# NewsSumm LED Fine-Tuning with AMD GPU (DirectML)\n",
    "# Single-file script\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_directml\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LEDForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "from bert_score import score\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SET DEVICE (AMD GPU via DirectML)\n",
    "# ---------------------------------------------------------\n",
    "device = torch_directml.device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. LOAD CLEANED NEWS SUMM DATASET\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_excel(\"newssumm_cleaned.xlsx\")\n",
    "\n",
    "# VERY IMPORTANT: start small (you can increase later)\n",
    "df = df.sample(2000, random_state=42)\n",
    "\n",
    "# Keep only required columns\n",
    "df = df[[\"article_text\", \"human_summary\"]]\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Train / validation split\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"test\"]\n",
    "\n",
    "print(\"Train samples:\", len(train_data))\n",
    "print(\"Validation samples:\", len(val_data))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. LOAD MODEL & TOKENIZER\n",
    "# ---------------------------------------------------------\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Required settings for LED\n",
    "model.config.use_cache = False\n",
    "model.to(device)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. PREPROCESS FUNCTION (NEW API â€“ NO ERROR)\n",
    "# ---------------------------------------------------------\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"article_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=batch[\"human_summary\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize\n",
    "train_tokenized = train_data.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names\n",
    ")\n",
    "\n",
    "val_tokenized = val_data.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. TRAINING ARGUMENTS (DIRECTML SAFE)\n",
    "# ---------------------------------------------------------\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./newssumm_led_results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,       # ONE epoch only\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    fp16=False,               # DirectML DOES NOT support fp16\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. TRAINER\n",
    "# ---------------------------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. TRAIN MODEL (THIS IS FINE-TUNING)\n",
    "# ---------------------------------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8. GENERATE SUMMARY (EVALUATION)\n",
    "# ---------------------------------------------------------\n",
    "model.eval()\n",
    "\n",
    "sample_article = val_data[0][\"article_text\"][:1500]\n",
    "reference_summary = val_data[0][\"human_summary\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    sample_article,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=200,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "generated_summary = tokenizer.decode(\n",
    "    summary_ids[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"\\nGENERATED SUMMARY:\\n\", generated_summary)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9. ROUGE SCORE\n",
    "# ---------------------------------------------------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "rouge_scores = rouge.compute(\n",
    "    predictions=[generated_summary],\n",
    "    references=[reference_summary]\n",
    ")\n",
    "\n",
    "print(\"\\nROUGE SCORES:\", rouge_scores)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 10. BERTScore (WHAT THEY WANT)\n",
    "# ---------------------------------------------------------\n",
    "P, R, F1 = score(\n",
    "    [generated_summary],\n",
    "    [reference_summary],\n",
    "    lang=\"en\",\n",
    "    model_type=\"microsoft/deberta-xlarge-mnli\"\n",
    ")\n",
    "\n",
    "print(\"\\nBERTScore F1:\", F1.mean().item())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
